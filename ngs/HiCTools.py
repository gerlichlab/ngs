"""A collections of functions to facilitate
analysis of HiC data based on the cooler and cooltools
interfaces."""
import warnings
from typing import Tuple, Dict, Callable
import cooltools.expected
import cooltools.snipping
import pandas as pd
import bioframe
import cooler
import pairtools
import numpy as np
import multiprocess
from .snipping_lib import flexible_pileup

# define type aliases

CisTransPairs = Dict[str, pd.DataFrame]
PairsSamples = Dict[str, CisTransPairs]

# define functions


def get_expected(
    clr: cooler.Cooler, arms: pd.DataFrame, proc: int = 20, ignore_diagonals: int = 2
) -> pd.DataFrame:
    """Takes a clr file handle and a pandas dataframe
    with chromosomal arms (generated by getArmsHg19()) and calculates
    the expected read number at a certain genomic distance.
    The proc parameters defines how many processes should be used
    to do the calculations. ingore_diags specifies how many diagonals
    to ignore (0 mains the main diagonal, 1 means the main diagonal
    and the flanking tow diagonals and so on)"""
    with multiprocess.Pool(proc) as pool:
        expected = cooltools.expected.diagsum(
            clr,
            tuple(arms.itertuples(index=False, name=None)),
            transforms={"balanced": lambda p: p["count"] * p["weight1"] * p["weight2"]},
            map=pool.map,
            ignore_diags=ignore_diagonals,
        )
    # construct a single dataframe for all regions (arms)
    expected_df = pd.concat(
        [
            exp.reset_index().assign(chrom=reg[0], start=reg[1], end=reg[2])
            for reg, exp in expected.items()
        ]
    )
    # aggregate diagonals over the regions specified by chrom, start, end (arms)
    expected_df = (
        expected_df.groupby(["chrom", "start", "end", "diag"])
        .aggregate({"n_valid": "sum", "count.sum": "sum", "balanced.sum": "sum"})
        .reset_index()
    )
    # account for different number of valid bins in diagonals
    expected_df["balanced.avg"] = expected_df["balanced.sum"] / expected_df["n_valid"]
    return expected_df


def get_arms_hg19() -> pd.DataFrame:
    """Downloads the coordinates for chromosomal arms of the
    genome assembly hg19 and returns it as a dataframe."""
    # download chromosomal sizes
    chromsizes = bioframe.fetch_chromsizes("hg19")
    # download centromers
    centromeres = bioframe.fetch_centromeres("hg19")
    centromeres.set_index("chrom", inplace=True)
    centromeres = centromeres.mid
    # define chromosomes that are well defined (filter out unassigned contigs)
    good_chroms = list(chromsizes.index[:23])
    # construct arm regions (for each chromosome fro 0-centromere and from centromere to the end)
    arms = [
        arm
        for chrom in good_chroms
        for arm in (
            (chrom, 0, centromeres.get(chrom, 0)),
            (chrom, centromeres.get(chrom, 0), chromsizes.get(chrom, 0)),
        )
    ]
    # construct dataframe out of arms
    arms = pd.DataFrame(arms, columns=["chrom", "start", "end"])
    return arms


def assign_regions(
    window: int,
    binsize: int,
    chroms: pd.Series,
    positions: pd.Series,
    arms: pd.DataFrame,
) -> pd.DataFrame:
    """Constructs a 2d region around a series of chromosomal location.
    Window specifies the windowsize for the constructed regions. The total region
    assigned will be pos-window until pos+window. The binsize specifies the size
    of the HiC bins. The positions which represent the center of the regions
    is givin the the chroms series and the positions series."""
    # construct windows from the passed chromosomes and positions
    snipping_windows = cooltools.snipping.make_bin_aligned_windows(
        binsize, chroms.values, positions.values, window
    )
    # assign chromosomal arm to each position
    snipping_windows = cooltools.snipping.assign_regions(
        snipping_windows, list(arms.itertuples(index=False, name=None))
    )
    return snipping_windows


def assign_regions_2d(
    window: int,
    binsize: int,
    chroms1: pd.Series,
    positions1: pd.Series,
    chroms2: pd.Series,
    positions2: pd.Series,
    arms: pd.DataFrame,
) -> pd.DataFrame:
    """Constructs a 2d region around a series of chromosomal location pairs.
    Window specifies the windowsize for the constructed regions. The total region
    assigned will be pos-window until pos+window. The binsize specifies the size
    of the HiC bins. The positions which represent the center of the regions
    is given by  the chroms1 and chroms2 series as well as the
    positions1 and positions2 series."""
    # construct windows from the passed chromosomes 1 and positions 1
    windows1 = assign_regions(window, binsize, chroms1, positions1, arms)
    windows1.columns = [str(i) + "1" for i in windows1.columns]
    # construct windows from the passed chromosomes 1 and positions 1
    windows2 = assign_regions(window, binsize, chroms2, positions2, arms)
    windows2.columns = [str(i) + "2" for i in windows2.columns]
    windows = pd.concat((windows1, windows2), axis=1)
    # concatenate windows
    windows = pd.concat((windows1, windows2), axis=1)
    # filter for mapping to different regions
    windows_final = windows.loc[windows["region1"] == windows["region2"], :]
    # subset data and rename regions
    windows_small = windows_final[
        ["chrom1", "start1", "end1", "chrom2", "start2", "end2", "region1"]
    ]
    windows_small.columns = [
        "chrom1",
        "start1",
        "end1",
        "chrom2",
        "start2",
        "end2",
        "region",
    ]
    return windows_small


def do_pileup_obs_exp(
    clr: cooler.Cooler,
    expected_df: pd.DataFrame,
    snipping_windows: pd.DataFrame,
    proc: int = 5,
    collapse: bool = True,
) -> np.ndarray:
    """Takes a cooler file handle, an expected dataframe
    constructed by getExpected, snipping windows constructed
    by assignRegions and performs a pileup on all these regions
    based on the obs/exp value. Returns a numpy array
    that contains averages of all selected regions.
    The collapse parameter specifies whether to return
    the average window over all piles (collapse=True), or the individual
    windows (collapse=False)."""
    oe_snipper = cooltools.snipping.ObsExpSnipper(clr, expected_df)
    # set warnings filter to ignore RuntimeWarnings since cooltools
    # does not check whether there are inf or 0 values in
    # the expected dataframe
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", RuntimeWarning)
        with multiprocess.Pool(proc) as pool:
            # extract a matrix of obs/exp average values for each snipping_window
            oe_pile = cooltools.snipping.pileup(
                snipping_windows, oe_snipper.select, oe_snipper.snip, map=pool.map
            )
    if collapse:
        # calculate the average of all windows
        collapsed_pile = np.nanmean(oe_pile[:, :, :], axis=2)
        return collapsed_pile
    return oe_pile


def do_pileup_iccf(
    clr: cooler.Cooler,
    snipping_windows: pd.DataFrame,
    proc: int = 5,
    collapse: bool = True,
) -> np.ndarray:
    """Takes a cooler file handle and snipping windows constructed
    by assignRegions and performs a pileup on all these regions
    based on the corrected HiC counts. Returns a numpy array
    that contains averages of all selected regions. The collapse
    parameter specifies whether to return
    the average window over all piles (collapse=True), or the individual
    windows (collapse=False)."""
    iccf_snipper = cooltools.snipping.CoolerSnipper(clr)
    with multiprocess.Pool(proc) as pool:
        iccf_pile = cooltools.snipping.pileup(
            snipping_windows, iccf_snipper.select, iccf_snipper.snip, map=pool.map
        )
    if collapse:
        # calculate the average of all windows
        collapsed_pile_plus = np.nanmean(iccf_pile[:, :, :], axis=2)
        return collapsed_pile_plus
    return iccf_pile


def sliding_diamond(
    array: np.ndarray, side_len: int = 6, center_x: bool = True
) -> Tuple[np.ndarray, np.ndarray]:
    """Will slide a diamond of side length 'sideLen'
    down the diagonal of the passed array and return
    the average values for each position and
    the relative position of each value with respect
    to the center of the array (in Bin units)"""
    # initialize accumulators for diamond value and x-position
    diamond_accumulator = list()
    bin_accumulator = list()
    if side_len % 2 == 0:
        half_window = side_len
        for i in range(0, (array.shape[0] - half_window + 1)):
            # extract diamond
            diamond_array = array[i : (i + half_window), i : (i + half_window)]
            # set inf to nan for calculation of mean
            diamond_array[np.isinf(diamond_array)] = np.nan
            diamond_accumulator.append(np.nanmean(diamond_array))
            # append x-value for this particular bin
            bin_accumulator.append(np.median(range(i, (i + half_window),)))
    else:
        half_window = side_len // 2
        for i in range(half_window, (array.shape[0] - half_window)):
            # extract diamond
            diamond_array = array[
                i - half_window : (i + half_window) + 1,
                i - half_window : (i + half_window) + 1,
            ]
            # set inf to nan for calculation of mean
            diamond_array[np.isinf(diamond_array)] = np.nan
            diamond_accumulator.append(np.nanmean(diamond_array))
            # append x-value for this particular bin
            bin_accumulator.append(
                np.median(range(i - half_window, (i + half_window) + 1,))
            )
    if center_x:
        x_out = np.array(bin_accumulator - np.median(bin_accumulator))
    else:
        x_out = np.array(bin_accumulator)
    return (x_out, np.array(diamond_accumulator))


def load_pairs(path: str) -> pd.DataFrame:
    """Function to load a .pairs or .pairsam file
    into a pandas dataframe.
    This only works for relatively small files!"""
    # get handels for header and pairs_body
    header, pairs_body = pairtools._headerops.get_header(
        pairtools._fileio.auto_open(path, "r")
    )
    # extract column names from header
    cols = pairtools._headerops.extract_column_names(header)
    # read data into dataframe
    frame = pd.read_csv(pairs_body, sep="\t", names=cols)
    return frame


def down_sample_pairs(
    sample_dict: PairsSamples, distance: int = 10 ** 4
) -> PairsSamples:
    """Will downsample cis and trans reads in sampleDict to contain
    as many combined cis and trans reads as the sample with the lowest readnumber of the
    specified distance. """
    # initialize output dictionary
    out_dict = {sample: {} for sample in sample_dict}
    for sample in sample_dict.keys():
        # create temporary dataframes
        cis_temp = sample_dict[sample]["cis"]
        cis_temp["rType"] = "cis"
        trans_temp = sample_dict[sample]["trans"]
        trans_temp["rType"] = "trans"
        # concatenate them and store in outdict
        out_dict[sample]["all"] = pd.concat((cis_temp, trans_temp))
        # filter on distance
        out_dict[sample]["all"] = out_dict[sample]["all"].loc[
            (out_dict[sample]["all"]["pos2"] - out_dict[sample]["all"]["pos1"])
            > distance,
            :,
        ]
    # get the minimum number of reads
    min_reads = min([len(i["all"]) for i in out_dict.values()])
    # do the downsampling and split into cis and trans
    for sample in out_dict.keys():
        out_dict[sample]["all"] = out_dict[sample]["all"].sample(n=min_reads)
        out_dict[sample]["cis"] = out_dict[sample]["all"].loc[
            out_dict[sample]["all"]["rType"] == "cis", :
        ]
        out_dict[sample]["trans"] = out_dict[sample]["all"].loc[
            out_dict[sample]["all"]["rType"] == "trans", :
        ]
        # get rid of all reads
        out_dict[sample].pop("all")
    return out_dict


def pile_to_frame(pile: np.ndarray) -> pd.DataFrame:
    """Takes a pile of pileup windows produced
    by doPileupsObsExp/doPileupsICCF (with collapse set to False;
    this is numpy ndarray with the following dimensions:
    pile.shape = [windowSize, windowSize, windowNumber])
    and arranges them as a dataframe with the pixels of the
    pile flattened into columns and each individual window
    being a row.
    Window1: | Pixel 1 | Pixel 2 | Pixel3| ...
    Window2: | Pixel 1 | Pixel 2 | Pixel3| ...
    Window3: | Pixel 1 | Pixel 2 | Pixel3| ...
    """
    return pd.DataFrame(
        pile.flatten().reshape(pile.shape[0] ** 2, pile.shape[2])
    ).transpose()


def get_diag_indices(arr: np.ndarray) -> list:
    """Helper function that returns the indices of the diagonal
    of a given array into a flattened representation of the array.
    For example, the 3 by 3 array:
    [0, 1, 2]
    [3, 4, 5]
    [6, 7, 8]
    would have diagonal indices [0, 4, 8].
    """
    assert arr.shape[0] == arr.shape[1], "Please supply a square array!"
    shape = arr.shape[0]
    return [
        i + index for index, i in enumerate(range(0, shape ** 2 - shape + 1, shape))
    ]


def get_pairing_score(
    clr: cooler.Cooler,
    windowsize: int = 4 * 10 ** 4,
    func: Callable = np.mean,
    regions: pd.DataFrame = pd.DataFrame(),
    norm: bool = True,
    blank_diag: bool = True,
    arms: pd.DataFrame = pd.DataFrame(),
) -> pd.DataFrame:
    """Takes a cooler file (clr),
    a windowsize (windowsize), a summary
    function (func) and a set of genomic
    regions to calculate the pairing score
    as follows: A square with side-length windowsize
    is created for each of the entries in the supplied genomics
    regions and the summary function applied to the Hi-C pixels
    at the location in the supplied cooler file. The results are
    returned as a dataframe. If no regions are supplied, regions
    are constructed for each bin in the cooler file to
    construct a genome-wide pairing score. Norm refers to whether the median of the
    calculated pairing score should be subtracted from the supplied values and blankDiag
    refers to whether the diagonal should be blanked before calculating pairing score."""
    # Check whether genomic regions were supplied
    if len(regions) == 0:
        # If no regions are supplied, pregenerate all bins; drop bins with nan weights
        regions = clr.bins()[:].dropna()
        # find midpoint of each bin to assign windows to each midpoint
        regions.loc[:, "mid"] = (regions["start"] + regions["end"]) // 2
    # check that norm is only set if genomewide pairingScore is calculated
    elif norm:
        raise ValueError("Norm flag can only be set with genomeWide pairingScore!")
    # drop nan rows from regions
    regions = regions.dropna()
    # fix indices
    regions.index = range(len(regions))
    regions.loc[:, "binID"] = range(len(regions))
    # Chromosomal arms are needed so each process only extracts a subset from the file
    if len(arms) == 0:
        arms = get_arms_hg19()
    # extract all windows
    windows = assign_regions(
        windowsize, clr.binsize, regions["chrom"], regions["mid"], arms
    )
    # add binID to later merge piles
    windows.loc[:, "binID"] = regions["binID"]
    windows = windows.dropna()
    # generate pileup
    pile = do_pileup_iccf(clr, windows, collapse=False)
    # convert to dataframe
    pile_frame = pile_to_frame(pile)
    if blank_diag:
        dummy_array = np.arange(pile[:, :, 0].shape[0] ** 2).reshape(
            pile[:, :, 0].shape[0], pile[:, :, 0].shape[0]
        )
        indices = get_diag_indices(dummy_array)
        pile_frame.iloc[:, indices] = np.nan
    # apply function to each row (row = individual window)
    summarized = pile_frame.apply(func, axis=1)
    # subset regions with regions that were assigned windows
    output = pd.merge(regions, windows, on="binID", suffixes=("", "_w")).dropna()
    # add results
    output.loc[:, "PairingScore"] = summarized
    # normalize by median
    if norm:
        output.loc[:, "PairingScore"] = output["PairingScore"] - np.median(
            output.dropna()["PairingScore"]
        )
    return output[["chrom", "start", "end", "PairingScore"]]


def get_pairing_score_obs_exp(
    clr: cooler.Cooler,
    expected: pd.DataFrame,
    windowsize: int = 4 * 10 ** 4,
    func: Callable = np.mean,
    regions: pd.DataFrame = pd.DataFrame(),
    norm: bool = True,
    arms: pd.DataFrame = pd.DataFrame(),
) -> pd.DataFrame:
    """Takes a cooler file (clr), an expected dataframe (expected; maybe generated by getExpected),
    a windowsize (windowsize), a summary
    function (func) and a set of genomic
    regions to calculate the pairing score
    as follows: A square with side-length windowsize
    is created for each of the entries in the supplied genomics
    regions and the summary function applied to the Hi-C pixels (obs/exp values)
    at the location in the supplied cooler file. The results are
    returned as a dataframe. If no regions are supplied, regions
    are constructed for each bin in the cooler file to
    construct a genome-wide pairing score."""
    # Check whether genomic regions were supplied
    if len(regions) == 0:
        # If no regions are supplied, pregenerate all bins; drop bins with nan weights
        regions = clr.bins()[:].dropna()
        # find midpoint of each bin to assign windows to each midpoint
        regions.loc[:, "mid"] = (regions["start"] + regions["end"]) // 2
    # check that norm is only set if genomewide pairingScore is calculated
    elif norm:
        raise ValueError("Norm flag can only be set with genomeWide pairingScore!")
    # drop nan rows from regions
    regions = regions.dropna()
    # fix indices
    regions.index = range(len(regions))
    regions.loc[:, "binID"] = range(len(regions))
    # Chromosomal arms are needed so each process only extracts a subset from the file
    if len(arms) == 0:
        arms = get_arms_hg19()
    # extract all windows
    windows = assign_regions(
        windowsize, clr.binsize, regions["chrom"], regions["mid"], arms
    )
    # add binID to later merge piles
    windows.loc[:, "binID"] = regions["binID"]
    windows = windows.dropna()
    # generate pileup
    pile = do_pileup_obs_exp(clr, expected, windows, collapse=False)
    # convert to dataframe
    pile_frame = pile_to_frame(pile)
    # replace inf with nan
    pile_frame = pile_frame.replace([np.inf, -np.inf], np.nan)
    # apply function to each row (row = individual window)
    summarized = pile_frame.apply(func, axis=1)
    # subset regions with regions that were assigned windows
    output = pd.merge(regions, windows, on="binID", suffixes=("", "_w")).dropna()
    # add results
    output.loc[:, "PairingScore"] = summarized
    # normalize by median
    if norm:
        output.loc[:, "PairingScore"] = output["PairingScore"] - np.median(
            output.dropna()["PairingScore"]
        )
    return output[["chrom", "start", "end", "PairingScore"]]


def extract_windows_different_sizes_iccf(regions, arms, cooler_file, processes=2):
    """For extraction of a collection of regions that span genomic regions .
    regions -> data_frame with chrom, start, end (start, end in genomic coordinates)
    cooler -> opened cooler file
    arms  -> chromosomal arms
    """
    # assign arms to regions
    snipping_windows = cooltools.snipping.assign_regions(
        regions, list(arms.itertuples(index=False, name=None))
    ).dropna()
    iccf_snipper = cooltools.snipping.CoolerSnipper(cooler_file)
    with multiprocess.Pool(processes) as pool:
        result = flexible_pileup(
            snipping_windows, iccf_snipper.select, iccf_snipper.snip, mapper=pool.map
        )
    return result
